---
title: "Goで作るセキュリティ分析LLMエージェント(23): エージェントのテスト"
emoji: "🧪"
type: "tech"
topics: ["go", "test", "ai"]
published: false
---

この記事はアドベントカレンダー「[Goで作るセキュリティ分析LLMエージェント](https://adventar.org/calendars/11354)」の23日目です。今回はLLMエージェントの品質保証をするためのテストについてです。LLMエージェントに関する品質保証はまだ発展途上の議論であり、筆者もまだ最適なアプローチを模索中です。

この記事が本アドベントカレンダーにおける最後の実装に関する解説になります。今回のコードは https://github.com/m-mizutani/leveret の [day23-test](https://github.com/m-mizutani/leveret/tree/day23-test) ブランチに格納されていますので適宜参照してください。


# 非決定的なLLMエージェントにおけるテストの課題

- LLMのテストは難しい
  - LLMはこれまでにないパラダイムをもたらしてくれた
    - これによってソフトウェアエンジニアリングにおいて実現できることの幅が一気に広がった
    - 厳密にロジックを定義しないでもfuzzyに処理できる
    - これまでのベストプラクティスやバッドノウハウがひっくり返り、全く新しい発想のアプリケーションもありえる
  - しかし同時にこれまでのパラダイムが通用しない部分もある
  - LLMエージェントのテストはもっと難しい
- (問題1) LLMは出力が非決定的、確率的に動作する
  - これまでのソフトウェアエンジニアリングの多くは決定性のあるロジックを前提に考えられているベストプラクティス等が多かった
    - 業務系アプリでは特にそう
    - 例えばテスト駆動開発とか
    - もちろんML系などでは以前から非決定的であったと思うが、LLMだとそれがさらに広がったと考えられる
      - ML系もものによっては学習データを固定すれば決定的な挙動を導くこともできる
  - LLMは非決定的に動作するため、テストの結果にどのような値が返ってくるかのボラが大きい
    - そのため値の一致を検証するようなタイプのテストとは相性が悪い
    - 回答方法を指定しても無視してくる場合がある
      - 例えば `yes` or `no` で答えろと言っているのに、 `maybe yes` みたいな答えを投げ込んでくるとか
      - これについてはスキーマを指定すれば一定解決するが、スキーマを使いたくない機能だと難しい
    - ボラを無くす方法として temperature を 0、top-pを1 にするという方法はあることはある
      - temperature は出力される文章のランダム性をどのくらい強くするかを制御するパラメータ
      - top-pは候補の確率の攻撃しきい値
      - temperature が0、top-pが1だと基本的には回答がほぼ固定される
    - しかし実際に使う場合はそういう使い方をしない場合が多い
      - ゆえにボラがあることを想定したうえでのテストというものが必要になる
- (問題2) 評価基準が曖昧である
  - テストとは「実行によって得られた結果と期待される結果に収まっているか」を検証するものである
  - 「LLMからの結果」というものをどのように表現するべきかが難しい
  - 期待する結果がテキストの場合様々なブレが起こり得る
    - 例えば「雪の色は何？」という質問の回答は「白」という意味が期待されるが、「白です」「それは白です」「It's white」のように意味的にはあっているが値として多様性がでてしまう
    - また、「赤、青、黄、白」から選んでといっても `"白"` とか `**白**` のような余計な装飾を入れてくるようなケースもある
- (問題3) 実行されるユースケースが幅広すぎる
  - 様々な状況、様々なデータ入力やツール実行が考えられる
  - ユーザがどういう入力をしてどういう出力を期待しているのかが千差万別すぎる
    - 入力される内容もそうだし、期待値もユーザ自身のメンタルモデルに左右される
  - そのためある入力に対して検証したとして、それで十分なのかというのがとてもいいにくい
    - ちょっと違う質問をするとまったく異なる動作をする可能性が全然ありうる
  - よって総合的な品質というのは実地の中から計測しようがない

# LLMエージェントの品質保証アプローチ

- セキュリティ分析エージェントに限らず考えられるアプローチをとりあえず挙げてみる
- ぬけもれある可能性はあるが、筆者はひとまずこの4つの分類で捉えている

## (1) LLMを使ったテスト評価

- テストによって生成された内容を、再度別のコンテキストのLLMで評価する方法
- これだと表記がぶれていたいとしても、期待した意味が返ってきているかを確認できる
- 長文になるようなものだったらEmbeddingを利用する方法も考えられる
- LLMを使った検証だとより抽象度の高い検査も可能になる
  - 例えば「この回答はいかに示すルールに従った内容を出力しているか？」など
  - モニタリング観点でスコアリングさせることもできる
- LLMに判定させている部分ではどのような内容に対してでもスキーマ出力を利用できる
  - 単語でも文章でも構造データでも、これまでやってきたようにJSON出力を指定してスキーマを指定できる
  - なのでテスト自体の構文も書きやすい
- これによってLLMが期待している答えを返しているかを検証できる
- デメリットは主に2つ
  - LLMを多重に使うことになる。これによってテストのレイテンシとコストの悪化が懸念される
    - ただし殆どの場合検証すべき項目は小さいのでレイテンシやコストの悪化は無視できる範囲
  - 結果検証のLLMの判定自体が間違える可能性も当然ありうる
    - なので一般的なリグレッションテストとかと同じ精度とまではいかない
    - とはいえそもそもLLMの挙動自体が非決定的なのでそれはのむひつようがある

## (2) タスクの振る舞い検証

- タスクの行動に対していくつかのチェックポイントを用意しそれを検証する
  - 例えばあるツールの呼び出しが発生したか、ツール呼び出しじに適切な引数が指定されたかなどをテスト検証項目とする
  - また最終的な結果も期待した値が含まれているかどうかなどをチェックする
    - これについては(1)と組み合わせてもよい
- ツールはmockなどにしておくことで、呼び出しの状況をカウントしたり意図した値を差し込むなどできる
  - あるいは専用のツールを作って処理させてもよい
  - これはエージェントの呼び出し（実装例ではchat処理）を検証するのか、それとも内部ワークフローだけ検証するのかなど目的によってことなる
- 全体的な品質を担保するというよりは、概ね期待したフローで動作しているかを検証する意味合いが強い
  - LLMそのものの動作を担保するというより、Plan & Executeのような複雑な挙動が開発者の予想通りに動いているかを検証する
  - Plan & Execute のような全体の実行フロー制御は度々変更する。特にLLMへのプロンプトを調整することが多い
  - そのためそれを更新した際にこれまでの動作が大きくずれ込むようなデグレが発生していないかを確認する

## (3) ベンチマーク

- 決まった入力と期待される答えが共通ですでに用意されており、そのタスクの完了精度を測るやりかた
  - いろいろなシステム・モデルで使えるように汎化されている
  - 例 https://aclanthology.org/2025.emnlp-main.314.pdf
- 研究分野などで定型化されたベンチマークなどがあればそれを使える
  - ただしその場合タスクは一般化されたものであり、セキュリティ分析のような特定の業務に特化したものというのは現状だとまだほぼないはず
- また入力と出力が非常に明確なタスク（例：ある学術問題の回答を得よ、みたいなやつ）はわかりやすい
  - ただし問題を解く過程が一定以上複雑でないとなかなか差が出にくい
  - どちらかというとモデル自体の性能評価によく使われる手法というイメージ

## (4) 本番での実行に基づく評価

- 実際に本番環境で動かしたデータをもとに検証する手法
- いくつかパターンがある
  - トレースする
    - LLM実行に関する実行ログを詳細に保存して評価する手法
    - LangChain系列だと[LangSmith](https://www.langchain.com/langsmith/observability)が有名
    - ほかにも https://www.confident-ai.com/blog/definitive-ai-agent-evaluation-guide など
  - A/Bテスト
    - 2つ回答を提示してどちらが良い回答かユーザ自身に判断させる。ChatGPTがよくやっているやつ
    - フィードバックをもとに随時リファインされる

# セキュリティ分析エージェントのテスト方針

- 小規模に動かすツールである
  - すなわち、大量にトラフィックが流れるようなサービスではない
  - そのため大規模にフィードバックをとるような手法は向かない
  - つまりトレースやA/Bテストのような品質保証方法はあまりむかない
- 多大にコストをかけて整備する類のものでもない
  - 組織内の特定人物が使うようなシステムであり、不特定多数に提供するサービスではない
    - 人間によるフィードバックのほうが良いかもしれない
- 一方でタスクやワークフローがある程度複雑であり、コードで組んだロジック（Plan & Executeみたいなやつ）の検証はしたい
  - 新しい機能を追加したことによるサイドエフェクトやデグレが発生していないかを確認する必要がある
- これらの要件を鑑みて、(2)タスクの振る舞い検証を(1)のLLMを使ったテスト評価と組み合わせてやる程度が妥当なのではというのが現状筆者の判断
  - もちろんベンチマークとか組んでもいいんだけど、それを都度ぶん回すにはLLM実行コストがそれなりにかかる
  - あとセキュリティ分析というタスクの性質を考えると、それにあわせたベンチマークを独自にがっつり用意する必要があり、メンテナンスも大変
  - 売り物だったら検討の余地はあるが、組織内とかで使うとかであれば過剰品質

# テスト実装

To be written

# まとめ

To be written
